{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b4da38",
   "metadata": {},
   "source": [
    "### Job Extraction Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ba71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "# Import helper functions\n",
    "from jobs_helper import scrape_single_config\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# File paths\n",
    "SITE_CONFIGS_PATH = './data/site_configs_raw.json'\n",
    "OUTPUT_JOBS_CSV = './data/avature_jobs.csv'\n",
    "OUTPUT_JOBS_JSON = './data/avature_jobs.json'\n",
    "\n",
    "# Scraping configuration\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Input: {SITE_CONFIGS_PATH}\")\n",
    "print(f\"Output: {OUTPUT_JOBS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab60210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load site configurations\n",
    "with open(SITE_CONFIGS_PATH, 'r') as f:\n",
    "    all_configs = json.load(f)\n",
    "\n",
    "df_configs = pd.DataFrame(all_configs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SITE CONFIGURATIONS LOADED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter for scrapable endpoints\n",
    "success_configs = df_configs[df_configs['status'] == 'success'].to_dict('records')\n",
    "partial_configs = df_configs[df_configs['status'] == 'partial'].to_dict('records')\n",
    "\n",
    "print(f\"\\nEndpoint Status:\")\n",
    "print(f\"  SUCCESS (paginated): {len(success_configs)}\")\n",
    "print(f\"  PARTIAL (HTML scrape): {len(partial_configs)}\")\n",
    "print(f\"  Total to scrape: {len(success_configs) + len(partial_configs)}\")\n",
    "\n",
    "print(f\"\\nSample SUCCESS endpoints:\")\n",
    "for config in success_configs[:3]:\n",
    "    print(f\"  {config['tenant']}: {config['endpoint']}\")\n",
    "\n",
    "if len(partial_configs) > 0:\n",
    "    print(f\"\\nSample PARTIAL endpoints:\")\n",
    "    for config in partial_configs[:3]:\n",
    "        print(f\"  {config['tenant']}: {config.get('sample_job_ids', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING JOB EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_jobs = []\n",
    "configs_to_scrape = success_configs + partial_configs\n",
    "\n",
    "print(f\"\\nProcessing {len(configs_to_scrape)} endpoints...\")\n",
    "print(f\"Estimated time: {len(configs_to_scrape) * 30 / 60:.1f} minutes\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    future_to_config = {\n",
    "        executor.submit(scrape_single_config, config): config \n",
    "        for config in configs_to_scrape\n",
    "    }\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_config):\n",
    "        config = future_to_config[future]\n",
    "        try:\n",
    "            jobs = future.result()\n",
    "            all_jobs.extend(jobs)\n",
    "            completed += 1\n",
    "            \n",
    "            if completed % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                jobs_so_far = len(all_jobs)\n",
    "                print(f\"\\nProgress: {completed}/{len(configs_to_scrape)} sites | \"\n",
    "                      f\"{jobs_so_far:,} jobs | {elapsed/60:.1f} min elapsed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Future failed for {config['tenant']}: {e}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRAPING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal time: {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"Total jobs collected: {len(all_jobs):,}\")\n",
    "print(f\"Sites scraped: {len(configs_to_scrape)}\")\n",
    "if len(configs_to_scrape) > 0:\n",
    "    print(f\"Average: {len(all_jobs)/len(configs_to_scrape):.1f} jobs/site\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51976f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.DataFrame(all_jobs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEDUPLICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "before_dedup = len(df_jobs)\n",
    "print(f\"\\nOriginal jobs collected: {before_dedup:,}\")\n",
    "\n",
    "# Step 1: Remove same-tenant duplicates\n",
    "df_jobs_step1 = df_jobs.drop_duplicates(subset=['tenant', 'job_id'], keep='first')\n",
    "removed_step1 = before_dedup - len(df_jobs_step1)\n",
    "print(f\"Step 1 - Same-tenant duplicates: {removed_step1:,} removed\")\n",
    "\n",
    "# Step 2: Remove cross-tenant duplicates\n",
    "df_jobs_step2 = df_jobs_step1.drop_duplicates(subset=['job_id'], keep='first')\n",
    "removed_step2 = len(df_jobs_step1) - len(df_jobs_step2)\n",
    "print(f\"Step 2 - Cross-tenant duplicates: {removed_step2:,} removed\")\n",
    "\n",
    "# Step 3: Remove URL duplicates\n",
    "df_jobs_final = df_jobs_step2.drop_duplicates(subset=['job_url'], keep='first')\n",
    "removed_step3 = len(df_jobs_step2) - len(df_jobs_final)\n",
    "print(f\"Step 3 - URL duplicates: {removed_step3:,} removed\")\n",
    "\n",
    "print(f\"\\nFinal result:\")\n",
    "print(f\"  Before: {before_dedup:,}\")\n",
    "print(f\"  After: {len(df_jobs_final):,}\")\n",
    "print(f\"  Removed: {before_dedup - len(df_jobs_final):,} ({(before_dedup - len(df_jobs_final))/before_dedup*100:.1f}%)\")\n",
    "\n",
    "df_jobs = df_jobs_final\n",
    "\n",
    "# Save files\n",
    "df_jobs.to_csv(OUTPUT_JOBS_CSV, index=False)\n",
    "print(f\"\\nSaved CSV: {OUTPUT_JOBS_CSV}\")\n",
    "\n",
    "with open(OUTPUT_JOBS_JSON, 'w') as f:\n",
    "    json.dump(df_jobs.to_dict('records'), f, indent=2)\n",
    "print(f\"Saved JSON: {OUTPUT_JOBS_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JOB EXTRACTION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOverall:\")\n",
    "print(f\"  Total jobs: {len(df_jobs):,}\")\n",
    "print(f\"  Unique tenants: {df_jobs['tenant'].nunique()}\")\n",
    "print(f\"  Jobs with descriptions: {df_jobs['job_description'].notna().sum():,} ({df_jobs['job_description'].notna().sum()/len(df_jobs)*100:.1f}%)\")\n",
    "print(f\"  Jobs with locations: {df_jobs['location'].notna().sum():,} ({df_jobs['location'].notna().sum()/len(df_jobs)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTop 10 Tenants by Job Count:\")\n",
    "top_tenants = df_jobs['tenant'].value_counts().head(10)\n",
    "for tenant, count in top_tenants.items():\n",
    "    print(f\"  {tenant}: {count:,} jobs\")\n",
    "\n",
    "print(f\"\\nSample jobs:\")\n",
    "print(df_jobs[['tenant', 'job_title', 'location']].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5be9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check completeness\n",
    "print(f\"\\nField Completeness:\")\n",
    "for col in ['job_title', 'job_url', 'job_id', 'location', 'job_description']:\n",
    "    completeness = df_jobs[col].notna().sum() / len(df_jobs) * 100\n",
    "    print(f\"  {col}: {completeness:.1f}%\")\n",
    "\n",
    "# Check for issues\n",
    "issues = []\n",
    "\n",
    "no_title = df_jobs[df_jobs['job_title'].isna()]\n",
    "if len(no_title) > 0:\n",
    "    issues.append(f\"{len(no_title)} jobs missing titles\")\n",
    "\n",
    "no_url = df_jobs[df_jobs['job_url'].isna()]\n",
    "if len(no_url) > 0:\n",
    "    issues.append(f\"{len(no_url)} jobs missing URLs\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\nIssues Found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  {issue}\")\n",
    "else:\n",
    "    print(f\"\\nNo critical issues found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
